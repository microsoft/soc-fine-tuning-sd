# Model settings
model_name: "stable-diffusion-v1-5"
num_inference_steps: 50

# Optimization settings
optimizer: "adamw"
lr: 3.0e-6
beta1: 0.9
beta2: 0.95
batch_size: 5
accum_grad_steps: 25
gradient_clip: 0.0 #100.0 #0.0 for no clipping
gradient_clip_algorithm: "norm"

# Training strategy
eta: 1.0
precision: "32-true"
max_epochs: 10 #100000
num_timesteps_to_load: 50
num_timesteps_to_load_train: 4

# Paths
save_dir: "checkpoints"
training_prompt_path: "prompt_files/refl_data.json"
validation_prompt_path: "prompt_files/benchmark_ir.json"

# Scheduler settings
scheduler: "linear_warmup"
warmup_steps: 20

# Model-specific settings
beta_start: 0.002
beta_end: 0.009
reward_multiplier: 10.0
guidance_reward_fn: "ImageReward"
per_sample_threshold_quantile: 0.9 #-1 #negative value to disable
learn_offset: false

# Gradient smoothing settings
smooth_gradients: false
smooth_samples: 20
smooth_noise_std: 0.02
smooth_clipping_quantile: 0.85

# Misc
use_tf32: true
seed: 0
wandb_project: "AM-SD15-final"
checkpoint_every_n_epochs: 1
val_check_interval: 0.1
resume_from_checkpoint: null
verbose: false
